{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet=pd.read_csv(\"../input_data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(temp):\n",
    "    a=np.array(temp)\n",
    "    unique, counts=np.unique(a,return_counts=True)\n",
    "    return dict(zip(unique,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEntropy(q):\n",
    "    if q==0 or q==1:\n",
    "        return 0\n",
    "    return -(q*math.log2(q) + (1-q)*math.log2(1-q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating gini index\n",
    "def calcGini(q):\n",
    "    return 2*q*(1-q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating misclassification\n",
    "def calcMisClass(q):\n",
    "    return min(q,1-q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcImpurity(q,iMeasure):\n",
    "    if(iMeasure==0):\n",
    "        return calcEntropy(q)\n",
    "    if(iMeasure==1):\n",
    "        return calcGini(q)\n",
    "    return calcMisClass(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classifyData(data):\n",
    "    for item in data:\n",
    "        temp=makeDict(data[item])\n",
    "        if(len(temp)>10):\n",
    "            x=len(temp)//10 + 1\n",
    "            listOfKeys=list(temp.keys())\n",
    "            newCol=[]\n",
    "            for row in data[item]:\n",
    "                ind=listOfKeys.index(row)\n",
    "                upperBorder=(ind//x + 1)*x\n",
    "                if(upperBorder>=len(temp)):\n",
    "                    upperBorder=-1\n",
    "                newCol.append(listOfKeys[upperBorder])\n",
    "            data[item]=newCol\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcBestAttr(impurity,data,iMeasure):\n",
    "    iGains={}\n",
    "    for col in data:\n",
    "        if(col==\"left\"):\n",
    "            continue\n",
    "        uValCount=makeDict(data[col])\n",
    "        wAvg=0\n",
    "        for category in uValCount:\n",
    "            a=data[(data[col]==category) & (data['left']==1)]\n",
    "            q=len(a)/uValCount[category]\n",
    "            wAvg+=((uValCount[category]/len(data))*calcImpurity(q,iMeasure))\n",
    "        iGains[col]=impurity-wAvg\n",
    "    try:\n",
    "        maxCategory = max(iGains.keys(), key=(lambda k: iGains[k]))\n",
    "    except:\n",
    "        return None\n",
    "    return maxCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,root,sample):\n",
    "    key=list(root.keys())[0]\n",
    "    if(key=='value'):\n",
    "        return root[key]\n",
    "    try:\n",
    "        root=root[key]\n",
    "        if sample[key] in root.keys():\n",
    "            return predict(model,root[sample[key]],sample)\n",
    "        else:\n",
    "            for k in root.keys():\n",
    "                if(sample[key]<=k):\n",
    "                    return predict(model,root[k],sample)\n",
    "            return 0\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictChars(data):\n",
    "    truePositive,falsePositive,trueNegative,falseNegative=0,0,0,0\n",
    "    for sample in data:\n",
    "        pred_label=predict(DTree,DTree,sample)\n",
    "        if(pred_label==sample['left']):\n",
    "            if(pred_label):\n",
    "                truePositive+=1\n",
    "            else:\n",
    "                trueNegative+=1\n",
    "        else:\n",
    "            if(pred_label):\n",
    "                falsePositive+=1\n",
    "            else:\n",
    "                falseNegative+=1\n",
    "    accuracy=(truePositive+trueNegative)*100/len(data)\n",
    "    recall=truePositive/(truePositive+falseNegative)\n",
    "    precision=truePositive/(truePositive+falsePositive)\n",
    "    f1score=2/((1/precision)+(1/recall))\n",
    "    print(\"Accuracy is : {}\".format(accuracy))\n",
    "    print(\"Recall is : {}\".format(recall))\n",
    "    print(\"precision is : {}\".format(precision))\n",
    "    print(\"f1score is : {}\".format(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData=dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=inputData.sample(frac=0.8)\n",
    "validationData=inputData.drop(trainingData.index)\n",
    "trainingData=classifyData(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTree(root,data,iMeasure):\n",
    "    a=len(data[data.left==1])\n",
    "    if(a==0 or a==len(data)):\n",
    "        root.update({'value':1 if a else 0})\n",
    "        return\n",
    "    q=a/len(data)\n",
    "    newNode=calcBestAttr(calcImpurity(q,iMeasure),data,iMeasure)\n",
    "    if(newNode is None):\n",
    "        b=len(data)-a\n",
    "        if(a>=b):\n",
    "            root.update({'value':1})\n",
    "        else:\n",
    "            root.update({'value':0})\n",
    "        return\n",
    "    root.update({newNode:{}})\n",
    "    root=root[newNode]\n",
    "    for category in np.unique(data[newNode]):\n",
    "        root.update({category:{}})\n",
    "        temp=data[data[newNode]==category]\n",
    "        makeTree(root[category],temp.drop(columns=[newNode]),iMeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Entropy as impurity measure...\n",
      "Training Finished!\n",
      "Predicting Labels...\n",
      "Accuracy is : 96.9306049822064\n",
      "Recall is : 0.9365671641791045\n",
      "precision is : 0.9348230912476723\n",
      "f1score is : 0.9356943150046599\n"
     ]
    }
   ],
   "source": [
    "DTree={}\n",
    "print(\"Training with Entropy as impurity measure...\")\n",
    "makeTree(DTree,trainingData,0)\n",
    "print(\"Training Finished!\")\n",
    "print(\"Predicting Labels...\")\n",
    "predictChars(validationData.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with GiniIndex as impurity measure...\n",
      "Training Finished!\n",
      "Predicting Labels...\n",
      "Accuracy is : 96.97508896797153\n",
      "Recall is : 0.9365671641791045\n",
      "precision is : 0.9365671641791045\n",
      "f1score is : 0.9365671641791046\n"
     ]
    }
   ],
   "source": [
    "DTree={}\n",
    "print(\"Training with GiniIndex as impurity measure...\")\n",
    "makeTree(DTree,trainingData,1)\n",
    "print(\"Training Finished!\")\n",
    "print(\"Predicting Labels...\")\n",
    "predictChars(validationData.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Misclassification as impurity measure...\n",
      "Training Finished!\n",
      "Predicting Labels...\n",
      "Accuracy is : 97.10854092526691\n",
      "Recall is : 0.9347014925373134\n",
      "precision is : 0.943502824858757\n",
      "f1score is : 0.9390815370196813\n"
     ]
    }
   ],
   "source": [
    "DTree={}\n",
    "print(\"Training with Misclassification as impurity measure...\")\n",
    "makeTree(DTree,trainingData,2)\n",
    "print(\"Training Finished!\")\n",
    "print(\"Predicting Labels...\")\n",
    "predictChars(validationData.to_dict('records'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
