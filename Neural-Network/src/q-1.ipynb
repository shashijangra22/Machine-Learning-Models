{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet=pd.read_csv(\"../input_data/apparel-trainval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=dataSet.sample(frac=0.8)\n",
    "validationData=dataSet.drop(trainingData.index)\n",
    "\n",
    "trainingLabels=np.array(trainingData['label'])\n",
    "validationLabels=np.array(validationData['label'])\n",
    "\n",
    "trainingData=np.array(trainingData.drop('label',axis=1))\n",
    "validationData=np.array(validationData.drop('label',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self,currentSize):\n",
    "        self.layerSize=currentSize\n",
    "        self.Weights=None\n",
    "        self.Bias=None\n",
    "        self.Z=None\n",
    "        self.Activations=None\n",
    "        self.ActFunction=None\n",
    "        self.dActFunction=None\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self,nLayers,layerSizes,hLayerActFunction):\n",
    "        \n",
    "        self.nLayers=nLayers\n",
    "        self.layers=[]\n",
    "        for x in range(self.nLayers):\n",
    "            self.layers.append(Layer(layerSizes[x]))\n",
    "        \n",
    "        for x in range(1,self.nLayers):\n",
    "            self.layers[x].Weights=np.random.randn(layerSizes[x],layerSizes[x-1])*0.01\n",
    "            self.layers[x].Bias=np.zeros((layerSizes[x],1))\n",
    "            self.layers[x].ActFunction=getattr(self,hLayerActFunction)\n",
    "            self.layers[x].dActFunction=getattr(self,\"der\"+hLayerActFunction)\n",
    "        \n",
    "        self.layers[-1].ActFunction=getattr(self,\"SIGMOID\")\n",
    "        \n",
    "    def forwardProp(self,X): # making the prediction on current weights\n",
    "        self.layers[0].Activations=X.T\n",
    "        for i in range(1,self.nLayers-1):\n",
    "            self.layers[i].Z=np.dot(self.layers[i].Weights,self.layers[i-1].Activations)+self.layers[i].Bias\n",
    "            self.layers[i].Activations=self.layers[i].ActFunction(self.layers[i].Z)\n",
    "        # for last layer\n",
    "        self.layers[-1].Z=np.dot(self.layers[-1].Weights,self.layers[-2].Activations)+self.layers[-1].Bias\n",
    "        self.layers[-1].Activations=self.layers[-1].ActFunction(self.layers[-1].Z)\n",
    "        \n",
    "    def cost(self,Y):\n",
    "        X=self.layers[-1].Activations\n",
    "        Y=self.makeVector(Y)\n",
    "        return -(np.sum(Y*np.log(X)+(1-Y)*(np.log(1-X))))/len(Y)\n",
    "    \n",
    "    def backProp(self,X,Y): # back propogating the error\n",
    "        \n",
    "        self.layers[-1].dZ=self.layers[-1].Activations-self.makeVector(Y)\n",
    "        self.layers[-1].dW=np.dot(self.layers[-1].dZ,self.layers[-2].Activations.T)\n",
    "        self.layers[-1].dB=(np.sum(self.layers[-1].dZ,axis=1,keepdims=True))/len(Y)\n",
    "        \n",
    "        for i in range(self.nLayers-2,0,-1):\n",
    "            self.layers[i].dZ=np.dot(self.layers[i+1].Weights.T,self.layers[i+1].dZ)*(self.layers[i].dActFunction(self.layers[i].Z))\n",
    "            self.layers[i].dW=np.dot(self.layers[i].dZ,self.layers[i-1].Activations.T)/len(Y)\n",
    "            self.layers[i].dB=(np.sum(self.layers[i].dZ,axis=1,keepdims=True))/len(Y)\n",
    "    \n",
    "        for i in range(1,self.nLayers):\n",
    "            self.layers[i].Weights-=0.001*(self.layers[i].dW)\n",
    "            self.layers[i].Bias-=0.001*(self.layers[i].dB)\n",
    "    \n",
    "    def derSIGMOID(self,z): # derivative of sigmoid function\n",
    "        s=self.SIGMOID(z)\n",
    "        return s*(1-s)\n",
    "    \n",
    "    def SOFTMAX(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def TANH(self,x):\n",
    "        e_2x=np.exp(2*x)\n",
    "        return (e_2x-1)/(e_2x+1)\n",
    "    \n",
    "    def derTANH(self,x):\n",
    "        s=self.TANH(x)\n",
    "        return 1-s**2\n",
    "    \n",
    "    def RELU(self,z):\n",
    "        return np.maximum(z, 0)\n",
    "    \n",
    "    def derRELU(self,z):\n",
    "        return np.greater(z, 0).astype(int)\n",
    "    \n",
    "    def SIGMOID(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def makeVector(self,Y):\n",
    "        labels=[]\n",
    "        for i in Y:\n",
    "            arr=np.zeros(10)\n",
    "            arr[i]=1\n",
    "            labels.append(arr)\n",
    "        return np.array(labels).T\n",
    "    \n",
    "    def fit(self,X,Y,batchSize): # gradient Descent\n",
    "        for i in range(40):\n",
    "            for j in range(0,len(X),batchSize):\n",
    "                tempX=X[j:j+batchSize,:]\n",
    "                tempY=Y[j:j+batchSize]\n",
    "                self.forwardProp(tempX)\n",
    "                self.backProp(tempX,tempY)\n",
    "            print(\"iteration: \",i)\n",
    "    def predict(self,X):\n",
    "        self.forwardProp(X)\n",
    "        predictions=self.SOFTMAX(self.layers[-1].Activations).T\n",
    "        return np.array([np.argmax(item) for item in predictions])\n",
    "    \n",
    "    def checkAccuracy(self,A,B):\n",
    "        predictions=self.predict(A)\n",
    "        c=0\n",
    "        for a,b in zip(predictions,B):\n",
    "            if a==b:\n",
    "                c+=1\n",
    "        return c*100/len(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with RELU...\n",
      "iteration:  0\n",
      "iteration:  1\n",
      "iteration:  2\n",
      "iteration:  3\n",
      "iteration:  4\n",
      "iteration:  5\n",
      "iteration:  6\n",
      "iteration:  7\n",
      "iteration:  8\n",
      "iteration:  9\n",
      "iteration:  10\n",
      "iteration:  11\n",
      "iteration:  12\n",
      "iteration:  13\n",
      "iteration:  14\n",
      "iteration:  15\n",
      "iteration:  16\n",
      "iteration:  17\n",
      "iteration:  18\n",
      "iteration:  19\n",
      "iteration:  20\n",
      "iteration:  21\n",
      "iteration:  22\n",
      "iteration:  23\n",
      "iteration:  24\n",
      "iteration:  25\n",
      "iteration:  26\n",
      "iteration:  27\n",
      "iteration:  28\n",
      "iteration:  29\n",
      "iteration:  30\n",
      "iteration:  31\n",
      "iteration:  32\n",
      "iteration:  33\n",
      "iteration:  34\n",
      "iteration:  35\n",
      "iteration:  36\n",
      "iteration:  37\n",
      "iteration:  38\n",
      "iteration:  39\n",
      "Saving Weights & Bias...\n",
      "Training with SIGMOID...\n",
      "iteration:  0\n",
      "iteration:  1\n",
      "iteration:  2\n",
      "iteration:  3\n",
      "iteration:  4\n",
      "iteration:  5\n",
      "iteration:  6\n",
      "iteration:  7\n",
      "iteration:  8\n",
      "iteration:  9\n",
      "iteration:  10\n",
      "iteration:  11\n",
      "iteration:  12\n",
      "iteration:  13\n",
      "iteration:  14\n",
      "iteration:  15\n",
      "iteration:  16\n",
      "iteration:  17\n",
      "iteration:  18\n",
      "iteration:  19\n",
      "iteration:  20\n",
      "iteration:  21\n",
      "iteration:  22\n",
      "iteration:  23\n",
      "iteration:  24\n",
      "iteration:  25\n",
      "iteration:  26\n",
      "iteration:  27\n",
      "iteration:  28\n",
      "iteration:  29\n",
      "iteration:  30\n",
      "iteration:  31\n",
      "iteration:  32\n",
      "iteration:  33\n",
      "iteration:  34\n",
      "iteration:  35\n",
      "iteration:  36\n",
      "iteration:  37\n",
      "iteration:  38\n",
      "iteration:  39\n",
      "Saving Weights & Bias...\n"
     ]
    }
   ],
   "source": [
    "for func in [\"RELU\",\"SIGMOID\"]:\n",
    "    model=NeuralNetwork(4,[trainingData.shape[1],100,100,10],func)\n",
    "    print(\"Training with \"+func+\"...\")\n",
    "    model.fit(trainingData,trainingLabels,100)\n",
    "    print(\"Saving Weights & Bias...\")\n",
    "    for i in range(1,model.nLayers):\n",
    "        np.save(\"Weight_\"+func+\"_\"+str(i)+\".npy\",model.layers[i].Weights)\n",
    "        np.save(\"Bias_\"+func+\"_\"+str(i)+\".npy\",model.layers[i].Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with RELU is:  88.01666666666667\n",
      "Accuracy with SIGMOID is:  83.55\n"
     ]
    }
   ],
   "source": [
    "for func in [\"RELU\",\"SIGMOID\"]:\n",
    "    model=NeuralNetwork(4,[trainingData.shape[1],100,100,10],func)\n",
    "    for i in range(1,model.nLayers):\n",
    "        model.layers[i].Weights=np.load(\"Weight_\"+func+\"_\"+str(i)+\".npy\")\n",
    "        model.layers[i].Bias=np.load(\"Bias_\"+func+\"_\"+str(i)+\".npy\")\n",
    "    print(\"Accuracy with \"+func+\" is: \",model.checkAccuracy(validationData,validationLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingData=np.array(pd.read_csv(\"../input_data/apparel-test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralNetwork(4,[testingData.shape[1],100,100,10],\"RELU\")\n",
    "for i in range(1,model.nLayers):\n",
    "    model.layers[i].Weights=np.load(\"Weight_\"+\"RELU\"+\"_\"+str(i)+\".npy\")\n",
    "    model.layers[i].Bias=np.load(\"Bias_\"+\"RELU\"+\"_\"+str(i)+\".npy\")\n",
    "predictions=model.predict(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output_data/2018202001_prediction.csv\", \"wb\") as f:\n",
    "    f.write(b'pLabel\\n')\n",
    "    np.savetxt(f, predictions.astype(int), fmt='%i', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
